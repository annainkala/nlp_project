{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3ca744ec-873a-4fdf-aff4-9c4ec232b50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/r3pnjytj3056tzyf_rpysh7h0000gn/T/ipykernel_44495/3486339604.py:13: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'chat', 'pm', 'hey', 'u', 'lol', 'wanna', 'guys', 'like', 'hi']\n",
      "['i', 'chat', 'pm', 'hey', 'u', 'wanna', 'lol', 'guys', 'im', 'like']\n",
      "['i', 'chat', 'pm', 'lol', 'hey', 'wanna', 'like', 'u', 'im', 'girls']\n",
      "['i', 'chat', 'pm', 'lol', 'u', 'hey', 'wanna', 'im', 'like', 'guys']\n",
      "['i', 'chat', 'pm', 'lol', 'wanna', 'hey', 'u', 'girls', 'like', 'im']\n",
      "['i', 'hey', 'chat', 'wanna', 'pm', 'lol', 'u', 'girls', 'like', 'song']\n",
      "['i', 'pm', 'chat', 'hey', 'lol', 'u', 'wanna', 'im', 'girls', 'good']\n",
      "['pm', 'i', 'u', 'chat', 'lol', 'hey', 'wanna', 'im', 'guys', 'girls']\n",
      "['i', 'pm', 'chat', 'lol', 'u', 'hey', 'wanna', 'im', 'girls', 'like']\n",
      "['i', 'chat', 'pm', 'wanna', 'hey', 'lol', 'like', 'u', 'im', 'girls']\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import nps_chat\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as pp\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import empath\n",
    "import sys\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "import datetime\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "corpus_teen = {\"words\":[], \"tagged_words\":[],\"tagged_sentences\":[], \"sentences\":[],\"topics\":[],\"dialogs\":[]}\n",
    "corpus_adult = {\"words\":[],\"tagged_words\":[],\"tagged_sentences\":[], \"sentences\":[],\"topics\":[], \"dialogs\":[]}\n",
    "\n",
    "class ChatCorpora:\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "    #personalpronoun, Proper noun, singular, Proper noun, plural,Possessive pronoun\n",
    "    pronouns = {\"NNP\", \"PRP\",\"NNP\", \"PRP$\"}\n",
    "    adjectives = {\"ADJ\"}\n",
    "\n",
    "    def __init__(self):\n",
    "       self._get_chat_data()\n",
    "            \n",
    "    \n",
    "    def _preprocess(self,list1):\n",
    "        \n",
    "        flat_list = []\n",
    "        if not isinstance(list1, list):\n",
    "            flat_list.append(list1)\n",
    "        else:\n",
    "            for sublist in list1:\n",
    "                if not isinstance(sublist, list):\n",
    "                    flat_list.append(sublist)\n",
    "                else:\n",
    "                    for item in sublist:\n",
    "                        flat_list.append(item)\n",
    "        \n",
    "        flat_list2 = [word for word in flat_list if re.match(r'[^\\W\\d]*$', word) and word[0] != 'JOIN' and word[0] != 'PART' and word[0] != 'ACTION' and word[0] not in stop_words ]\n",
    "        return flat_list2\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_chat_data(self):\n",
    "       \n",
    "        for i, p in enumerate(nps_chat.xml_posts()):\n",
    "           \n",
    "            if p.get(\"user\").find(\"teen\") != -1 :\n",
    "                \n",
    "                corpus_teen[\"tagged_sentences\"].append(list((t.get(\"word\"), t.get(\"pos\")) for t in p[0])) \n",
    "            else:             \n",
    "                corpus_adult[\"tagged_sentences\"].append(list((t.get(\"word\"), t.get(\"pos\")) for t in p[0]))  \n",
    "\n",
    "\n",
    "        \n",
    "        for sen in corpus_teen[\"tagged_sentences\"]:\n",
    "            for word in sen:\n",
    "                \n",
    "                if re.match(r'[^\\W\\d]*$', word[0]) and word[0] != 'JOIN' and word[0] != 'PART' and word[0] != 'ACTION' and word[0] not in stop_words :                \n",
    "                    corpus_teen[\"tagged_words\"].append(word)\n",
    "                    corpus_teen[\"words\"].append(word[0].lower())\n",
    "  \n",
    "        for sen in corpus_adult[\"tagged_sentences\"]:\n",
    "            for word in sen:\n",
    "                if re.match(r'[^\\W\\d]*$', word[0]) and word[0] != 'JOIN' and word[0] != 'PART' and word[0] != 'ACTION' and word[0] not in stop_words:\n",
    "                    corpus_adult[\"tagged_words\"].append(word)\n",
    "                    corpus_adult[\"words\"].append(word[0].lower())\n",
    "\n",
    "\n",
    "        \n",
    "        sentence_teen = {\"sentence\":\"\",\"pronouns\":0, \"adjectives\": 0}\n",
    "        \n",
    "        for sen in corpus_teen[\"tagged_sentences\"]:\n",
    "            if re.match(r'[^\\W\\d]*$', sen[0][0]) and sen[0][0] != 'JOIN' and sen[0][0] != 'PART' and sen[0][0] != 'ACTION'   and word[0] not in stop_words:\n",
    "                sentence_teen[\"sentence\"] = sen\n",
    "                pro = 0\n",
    "                adj = 0\n",
    "                for word in sen:\n",
    "                    if word[1] in self.pronouns:\n",
    "                        pro +=1\n",
    "                    if word[1] in self.adjectives:\n",
    "                        adj +=1\n",
    "                corpus_teen[\"sentences\"].append({\"sentence\":sen,\"pronouns\":pro, \"adjectives\": adj})\n",
    "              \n",
    "        sentence_adult = {\"sentence\":\"\",\"pronouns\":0, \"adjectives\": 0}\n",
    "        \n",
    "        for sen in corpus_adult[\"tagged_sentences\"]:\n",
    "            if re.match(r'[^\\W\\d]*$', sen[0][0]) and sen[0][0] != 'JOIN' and sen[0][0] != 'PART' and sen[0][0] != 'ACTION'  and word[0] not in stop_words :\n",
    "                sentence_adult[\"sentence\"] = sen\n",
    "                pro = 0\n",
    "                adj = 0\n",
    "                for word in sen:\n",
    "                    if word[1] in self.pronouns:\n",
    "                        pro +=1\n",
    "                    if word[1] in self.adjectives:\n",
    "                        adj +=1\n",
    "                corpus_adult[\"sentences\"].append({\"sentence\":sen,\"pronouns\":pro, \"adjectives\": adj})\n",
    "     \n",
    "               \n",
    "\n",
    "class ZipfFrequenceData:\n",
    "\n",
    "    \n",
    "    def get_frequence_data_words(self, corpus):\n",
    "        \n",
    "        distribution = defaultdict(int)\n",
    "        \n",
    "\n",
    "        for word in corpus:\n",
    "            distribution[word]+=1\n",
    "        \n",
    "        np.seterr(divide = 'ignore') \n",
    "        distribution1 = sorted(distribution.items(), key=lambda x:x[1], reverse = True)\n",
    "        max_val = distribution1[0][1]\n",
    "  \n",
    "        distribution = {}\n",
    "      \n",
    "        for val in distribution1:\n",
    "            distribution[val[0]] = val[1]/max_val     \n",
    "        return distribution\n",
    "\n",
    "    \n",
    "    def get_frequence_data_sentence(self,corpus_dict):\n",
    "        \n",
    "        distribution = defaultdict(int)\n",
    "\n",
    "        for sen in corpus_dict:\n",
    "            distribution[str(sen[\"sentence\"])] = len(sen[\"sentence\"])\n",
    "            \n",
    "       \n",
    "        np.seterr(divide = 'ignore') \n",
    "        distribution1 = sorted(distribution.items(), key=lambda x:x[1], reverse = True)\n",
    "        max_val = distribution1[0][1]\n",
    "  \n",
    "        distribution = {}\n",
    "      \n",
    "        for val in distribution1:\n",
    "            distribution[val[0]] = val[1]/max_val     \n",
    "        return distribution\n",
    "\n",
    "\n",
    "    def get_frequence_data_pronouns(self,corpus_dict):\n",
    "        distribution = defaultdict(int)\n",
    "\n",
    "        for sen in corpus_dict:\n",
    "            distribution[str(sen[\"sentence\"])] = sen[\"pronouns\"]\n",
    "\n",
    "        \n",
    "            \n",
    "       \n",
    "        np.seterr(divide = 'ignore') \n",
    "        distribution1 = sorted(distribution.items(), key=lambda x:x[1], reverse = True)\n",
    "        max_val = distribution1[0][1]\n",
    "  \n",
    "        distribution = {}\n",
    "      \n",
    "        for val in distribution1:\n",
    "            distribution[val[0]] = val[1]/max_val     \n",
    "        return distribution\n",
    "\n",
    "    def get_frequence_data_adjectives(self,corpus_dict):\n",
    "        distribution = defaultdict(int)\n",
    "\n",
    "        for sen in corpus_dict:\n",
    "            distribution[str(sen[\"sentence\"])] = sen[\"adjectives\"]\n",
    "\n",
    "        \n",
    "            \n",
    "       \n",
    "        np.seterr(divide = 'ignore') \n",
    "        distribution1 = sorted(distribution.items(), key=lambda x:x[1], reverse = True)\n",
    "        max_val = distribution1[0][1]\n",
    "  \n",
    "        distribution = {}\n",
    "      \n",
    "        for val in distribution1:\n",
    "            distribution[val[0]] = val[1]/max_val     \n",
    "        return distribution\n",
    "        \n",
    "\n",
    "    def get_zipf_data(self,count):\n",
    "        result = []\n",
    "        for i in range(count):\n",
    "            result.append(1/(i +1))\n",
    "        return result\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "class ShowData:\n",
    "    \n",
    "    def _get_prediction_interval(self, model_line_point, y_data, model_line, pi=.90):\n",
    "       \n",
    "        \n",
    "        sum_errs = np.sum((y_data - model_line)**2)\n",
    "        stdev = np.sqrt(1 / (len(y_data) - 2) * sum_errs)\n",
    "        one_minus_pi = 1 - pi\n",
    "        ppf_lookup = 1 - (one_minus_pi / 2)\n",
    "        z_score = stats.norm.ppf(ppf_lookup)\n",
    "        interval = z_score * stdev\n",
    "\n",
    "        lower, upper = model_line_point - interval, model_line_point + interval\n",
    "        return lower, model_line_point, upper\n",
    " \n",
    "\n",
    "   \n",
    "\n",
    "    def _tokens_outside_bounds(self, data, upper_interval, lower_interval):\n",
    "\n",
    "        data2 = []\n",
    "        sum2 = 0\n",
    "        for d in data:\n",
    "            if (data < upper_interval)| (data > lower_interval):\n",
    "                data2.append(d)\n",
    "                sum2 += d\n",
    "\n",
    "        outside_values = sum2\n",
    "        return outside_values/len(data) * 100\n",
    "        \n",
    "    \n",
    "    \n",
    "    def show_data(self, distribution, zipf):\n",
    "  \n",
    "        \n",
    "        data_y = list(distribution.values())\n",
    "        data_x = np.arange(1,len(data_y) + 1,1)\n",
    "      \n",
    "\n",
    "      \n",
    "        lower_vet = []\n",
    "        upper_vet = []\n",
    "        \n",
    "       \n",
    "        log_x = np.log10(data_x)\n",
    "        log_y = np.log10(data_y)\n",
    "        \n",
    "        \n",
    "        log_x_composed = log_x.reshape(-1,1)\n",
    "       \n",
    "        lin_regression = LinearRegression().fit(log_x_composed, log_y)\n",
    "        model_line = lin_regression.predict(log_x_composed)\n",
    "        \n",
    "        \n",
    "        for m in model_line:\n",
    "            lower, prediction, upper =  self._get_prediction_interval(m,log_y, model_line)\n",
    "            lower_vet.append(lower)\n",
    "            upper_vet.append(upper)\n",
    "    \n",
    "        \n",
    "        plt.figure\n",
    "        plt.plot(log_x,log_y)\n",
    "        plt.plot(log_x, model_line)\n",
    "        print(len(log_x), len(upper_vet), len(lower_vet))\n",
    "        plt.fill_between(log_x,upper_vet,lower_vet , color='b',label='Confidence Interval')\n",
    "        #plt.plot(np.arange(0,len(y_data),1),y_data,color='orange',label='Real data')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data = ZipfFrequenceData().get_frequence_data_words(corpus_adult[\"words\"])\n",
    "#data = ZipfFrequenceData().get_frequence_data_sentence(corpus_adult[\"sentences\"])\n",
    "#zipf = ZipfFrequenceData().get_zipf_data(len(data))\n",
    "#ShowData().show_data(data,zipf)\n",
    "\n",
    "\n",
    "\n",
    "#3. Wordcloud\n",
    "\n",
    "\n",
    " \n",
    "# Reads 'Youtube04-Eminem.csv' file \n",
    "\n",
    "\n",
    "class ShowWordCloud:\n",
    "    def show_word_cloud(self, corpus):\n",
    "        stopwords = set(STOPWORDS)\n",
    "\n",
    "     \n",
    "        comment_words = \" \".join(corpus)+\" \"\n",
    "         \n",
    "        wordcloud = WordCloud(width = 800, height = 800,\n",
    "                        background_color ='white',\n",
    "                        stopwords = stopwords,\n",
    "                        min_font_size = 10).generate(comment_words)\n",
    "         \n",
    "        # plot the WordCloud image                       \n",
    "        plt.figure(figsize = (8, 8), facecolor = None)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad = 0)\n",
    "         \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#ShowWordCloud().show_word_cloud(corpus_adult[\"words\"])\n",
    "\n",
    "#4\n",
    "class EmpathClient:\n",
    "\n",
    "    def show_empath_tables(self):\n",
    "        lexicon = empath.Empath()\n",
    "    \n",
    "        adult_chat = corpus_adult[\"words\"]\n",
    "        teen_chat = corpus_teen[\"words\"]\n",
    "        \n",
    "        adult_categories = lexicon.analyze(adult_chat, normalize=True)\n",
    "        teen_categories = lexicon.analyze(teen_chat, normalize=True)\n",
    "        \n",
    "        def top_categories(categories, n=10):\n",
    "          return sorted(categories.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "        \n",
    "        #print(top_categories(adult_categories, 30))\n",
    "        #print(top_categories(teen_categories, 30))\n",
    "        adult_data = (top_categories(adult_categories, 30))\n",
    "        teen_data = (top_categories(teen_categories, 30))\n",
    "       \n",
    "        df_teen = pd.DataFrame(teen_data, columns = [\"categories\", \"freuency\"])\n",
    "        df_adult = pd.DataFrame(adult_data, columns = [\"categories\", \"freuency\"])\n",
    "\n",
    "       \n",
    "     \n",
    "        #print(df_teen)\n",
    "        #print(df_adult)\n",
    "\n",
    "        print(df_teen.compare(df_adult))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#EmpathClient().show_empath_tables()\n",
    "\n",
    "#5\n",
    "\n",
    "class LDAModel:\n",
    "   \n",
    "    def print_keywords(self,num_of_topics):\n",
    "        dataset = [list(corpus_teen[\"words\"])]\n",
    "        #print(dataset)\n",
    "        data_words = dataset#[d.split() for d in dataset]\n",
    "        #print(data_words)\n",
    "        #print(data_words[:1][0][:30])\n",
    "        #print(dataset)\n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "        # Create Corpus\n",
    "        texts = data_words\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "        # View\n",
    "        #print(corpus[:1][0][:30])\n",
    "        \n",
    "        \n",
    "        # number of topics\n",
    "        num_topics = num_of_topics\n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics)\n",
    "        topic_list = []\n",
    "        # Print the Keyword in the 10 topics\n",
    "        for topic in lda_model.print_topics():\n",
    "            list1 = []\n",
    "            topics0 = topic[1].split('+')\n",
    "            for topic0 in topics0:\n",
    "                topic1 = topic0.split('*')\n",
    "                #print(topic1[1])\n",
    "                topic1[1] = topic1[1].replace('\"','')\n",
    "                list1.append(topic1[1].strip())\n",
    "            \n",
    "            \n",
    "            topic_list.append(list1)\n",
    "        t_dict = {}\n",
    "       \n",
    "          \n",
    "        corpus_teen[\"topics\"] = topic_list\n",
    "        doc_lda = lda_model[corpus]\n",
    "        #self.show_LDA_data( lda_model, corpus, id2word, num_topics)\n",
    "        #print(corpus_teen[\"topics\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def show_LDA_data(self, lda_model, corpus, id2word, num_topics):\n",
    "        \n",
    "           \n",
    "        # Visualize the topics\n",
    "        pyLDAvis.enable_notebook(local=True)\n",
    "        LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "        # # this is a bit time consuming - make the if statement True\n",
    "        # # if you want to execute visualization prep yourself\n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "        '''\n",
    "        if 1 == 1:\n",
    "            LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "            with open(LDAvis_data_filepath, 'wb') as f:\n",
    "                pickle.dump(LDAvis_prepared, f)\n",
    "        # load the pre-prepared pyLDAvis data from disk\n",
    "        with open(LDAvis_data_filepath, 'rb') as f:\n",
    "            LDAvis_prepared = pickle.load(f)\n",
    "        pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "        '''\n",
    "        display(LDAvis_prepared)\n",
    "\n",
    "\n",
    "#6\n",
    "class VaderScore:\n",
    "    \n",
    "    def display_vader_score(self):\n",
    "        LDAModel().print_keywords(10)\n",
    "        \n",
    "        text = corpus_teen[\"topics\"]\n",
    "        if text == []:\n",
    "            text = LDAModel().print_keywords(10)\n",
    "        \n",
    "        df = text\n",
    "        text2 = []\n",
    "        for t in text:\n",
    "            t = ' '.join(t)\n",
    "            text2.append(t)\n",
    "            #print(t)\n",
    "        #print(text2)\n",
    "        df = text2\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        vader_keywords = []\n",
    "        vader_score = []\n",
    "        \n",
    "        for t in text2:\n",
    "            #list2.append((t,sid.polarity_scores(t)))\n",
    "            #vader.append([t, sid.polarity_scores(t)])\n",
    "            vader_keywords.append(t)\n",
    "            vader_score.append(sid.polarity_scores(t))\n",
    "            #vader.append(vader)\n",
    "        #print(vader)\n",
    "        df_vader= pd.DataFrame( {\"keywords\": vader_keywords,\"score\": vader_score})\n",
    "        #print(df_vader)\n",
    "        display(df_vader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#7\n",
    "class CoherenceOfCorpus:\n",
    "\n",
    "    \n",
    "    def coherence(self):\n",
    "        ChatCorpora()\n",
    "        corpus = corpus_teen[\"words\"]\n",
    "        LDAModel().print_keywords(10)\n",
    "        topics_list = corpus_teen[\"topics\"]\n",
    "        \n",
    "        for topic in topics_list:\n",
    "            print(topic)\n",
    "        \n",
    "        word_coherence = {}\n",
    "        for word in corpus:\n",
    "            word_coherence[word] = 0\n",
    "\n",
    "        corpus1 = list(set(corpus))\n",
    "        for word in corpus1:\n",
    "            \n",
    "            for i, topics in enumerate(topics_list):\n",
    "                  \n",
    "                if word.strip() in list(set(topics_list[i])):\n",
    "                    word_coherence[word]+=1\n",
    "        #print(word_coherence.keys())\n",
    "        values = list(word_coherence.values())\n",
    "        values2 = []\n",
    "        for v in values:\n",
    "            v = (v-1)/10\n",
    "            values2.append(v)\n",
    "        #print(values2)\n",
    "        max1 = max(values2)\n",
    "        print(max1)\n",
    "        #print(word_coherence)\n",
    "\n",
    "        coherence = 1-max1\n",
    "        return coherence\n",
    "        \n",
    "                    \n",
    "                                    \n",
    "                        \n",
    "              \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#8\n",
    "class ShowDialogAct:\n",
    "\n",
    "    def __init__(self):\n",
    "       \n",
    "        \n",
    "        self.dialogs_teen ={\"Emotion\":0, \"ynAnswer\":0, \"Continuer\":0, \"WhQuestion\":0, \"System\":0, \"Accept\":0, \"Clarity\":0\n",
    "                     , \"Emphasis\":0, \"nAnswer\":0, \"Great\":0, \"Statement\":0, \"Reject\":0, \"Bye\":0, \"Others\":0\n",
    "            }\n",
    "        self.dialogs_adult ={\"Emotion\":0, \"ynAnswer\":0, \"Continuer\":0, \"WhQuestion\":0, \"System\":0, \"Accept\":0, \"Clarity\":0\n",
    "                     , \"Emphasis\":0, \"nAnswer\":0, \"Great\":0, \"Statement\":0, \"Reject\":0, \"Bye\":0, \"Others\":0\n",
    "            }\n",
    "    \n",
    "    \n",
    "    def show_dialog_act(self):\n",
    "        #print(corpus_teen[\"sentences\"])\n",
    "        for i, p in enumerate(nps_chat.xml_posts()):\n",
    "               \n",
    "            if p.get(\"user\").find(\"teen\") != -1 :\n",
    "                da = p.get(\"class\")\n",
    "                test = da in self.dialogs_teen.keys()\n",
    "                #print(dialogs_teen.keys(), da, test )\n",
    "                if da in self.dialogs_teen.keys():\n",
    "                    self.dialogs_teen[da] +=1\n",
    "                    #if da == \"Bye\":\n",
    "                        #print(da, dialogs_teen[da])\n",
    "            else:\n",
    "                da = p.get(\"class\")\n",
    "                test = da in self.dialogs_adult.keys()\n",
    "                    #print(dialogs_teen.keys(), da, test )\n",
    "                if da in self.dialogs_adult.keys():\n",
    "                    self.dialogs_adult[da] +=1\n",
    "                    #if da == \"Bye\":\n",
    "                            #print(da, dialogs_adult[da])\n",
    "\n",
    "        \n",
    "        print(self.dialogs_teen.values())\n",
    "        plt.figure\n",
    "        plt.bar(self.dialogs_teen.keys(),self.dialogs_teen.values(),color ='red',\n",
    "        width = 0.4)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "        print(self.dialogs_adult.values())\n",
    "        plt.figure\n",
    "        plt.bar(self.dialogs_adult.keys(),self.dialogs_adult.values(),color ='blue',\n",
    "        width = 0.4)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "        corpus_teen[\"dialogs\"] = self.dialogs_teen\n",
    "        corpus_adult[\"dialogs\"] = self.dialogs_adult\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "teen_dates = {}\n",
    "adult_dates = {}\n",
    "teen_dates_vocabulary = {}\n",
    "teen_vocabulary_set = set()\n",
    "\n",
    "#9 & 10\n",
    "class TimelyDescretion:\n",
    "\n",
    "    def _get_prediction_interval(self, model_line_point, y_data, model_line, pi=.90):\n",
    "       \n",
    "        \n",
    "        sum_errs = np.sum((y_data - model_line)**2)\n",
    "        stdev = np.sqrt(1 / (len(y_data) - 2) * sum_errs)\n",
    "        one_minus_pi = 1 - pi\n",
    "        ppf_lookup = 1 - (one_minus_pi / 2)\n",
    "        z_score = stats.norm.ppf(ppf_lookup)\n",
    "        interval = z_score * stdev\n",
    "\n",
    "        lower, upper = model_line_point - interval, model_line_point + interval\n",
    "        return lower, model_line_point, upper\n",
    " \n",
    "\n",
    "\n",
    "    def _preprocess(self, list1):\n",
    "        \n",
    "        flat_list = []\n",
    "        if not isinstance(list1, list):\n",
    "            flat_list.append(list1)\n",
    "        else:\n",
    "            for sublist in list1:\n",
    "                if not isinstance(sublist, list):\n",
    "                    flat_list.append(sublist)\n",
    "                else:\n",
    "                    for item in sublist:\n",
    "                        flat_list.append(item)\n",
    "            \n",
    "        flat_list2 = [word for word in flat_list if re.match(r'[^\\W\\d]*$', word) and word != 'JOIN' and word != 'PART' and word not in \n",
    "                      stop_words]\n",
    "        return flat_list2\n",
    "\n",
    "   \n",
    "    \n",
    "    def get_interval(self,list1, b, c):\n",
    "        list1 = sorted(list1)\n",
    "        d1 = datetime.datetime(2006, 1, 1).timestamp()\n",
    "        d2 = datetime.datetime(2006, 1, 2) .timestamp()\n",
    "        i0 = (d2-d1)\n",
    "\n",
    "        \n",
    "        t_min = list1[0] -b*i0\n",
    "        t_max = list1[-1] -b*i0\n",
    "       \n",
    "        interval =  c*i0\n",
    "        return t_min, t_max,interval\n",
    "\n",
    "    \n",
    "    def timely_descretion(self):\n",
    "        for p in nps_chat.xml_posts():\n",
    "            \n",
    "            datetime1 = datetime.datetime(2006, int(p.get(\"user\")[0:2]), int(p.get(\"user\")[3:5])).timestamp() \n",
    "            if p.get(\"user\").find(\"teen\") != -1 :\n",
    "            \n",
    "                teen_dates[datetime1] = 0\n",
    "                teen_dates_vocabulary[datetime1] = 0\n",
    "            else:\n",
    "                adult_dates[datetime1] = 0\n",
    "            \n",
    "\n",
    "   \n",
    "        for p in nps_chat.xml_posts():\n",
    "            \n",
    "            datetime1 = datetime.datetime(2006, int(p.get(\"user\")[0:2]), int(p.get(\"user\")[3:5])).timestamp()\n",
    "            if p.get(\"user\").find(\"teen\") != -1 :\n",
    "            \n",
    "                list1 = list(t.get(\"word\") for t in p[0]  if re.match(r'[^\\W\\d]*$', t.get(\"word\") ) \n",
    "                             and t.get(\"word\") != 'JOIN' and t.get(\"word\")  != 'PART' and t.get(\"word\")  != 'ACTION' and word not in stop_words)\n",
    "            \n",
    "                if list1 != []:\n",
    "                    list2 = self._preprocess(list1)\n",
    "                    \n",
    "                    teen_dates[datetime1] += len(list2)\n",
    "                    for t in list2:\n",
    "                        if t not in teen_vocabulary_set:\n",
    "                        \n",
    "                            teen_dates_vocabulary[datetime1] += 1\n",
    "                            teen_vocabulary_set.add(t) \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    \n",
    "               \n",
    "            else:\n",
    "               pass\n",
    "                \n",
    "            \n",
    "        startdate = 0\n",
    "        dayinterval = 7\n",
    "        #sorted_teen_dates = sorted(list(teen_dates_vocabulary.keys()))\n",
    "        min0,max0, interval = self.get_interval(teen_dates,startdate,dayinterval)\n",
    "       \n",
    "        \n",
    "        interval_list_teen = {}\n",
    "        interval_list_teen_vocabulary = {}\n",
    "        interval_values = 0\n",
    "        key0 = min0\n",
    "        while key0 < max0 :\n",
    "            interval_list_teen[key0] = 0\n",
    "            interval_list_teen_vocabulary[key0] = 0\n",
    "            key0 += interval\n",
    "\n",
    "        interval_keys = list(interval_list_teen.keys())\n",
    "        print(interval_keys)\n",
    "        \n",
    "        for d in teen_dates.keys():\n",
    "            for i, key  in enumerate(interval_keys):\n",
    "                \n",
    "                if i < len(interval_keys) -1:\n",
    "                    if d >= interval_keys[i] and d < interval_keys[i+1]:\n",
    "                    \n",
    "                        interval_list_teen[interval_keys[i]] += teen_dates[d]\n",
    "\n",
    "        for d in teen_dates_vocabulary.keys():\n",
    "            for i, key  in enumerate(interval_keys):\n",
    "                if i < len(interval_keys) -1:\n",
    "                    if d >= interval_keys[i] and d < interval_keys[i+1]:\n",
    "                        interval_list_teen_vocabulary[interval_keys[i]] += teen_dates_vocabulary[d]\n",
    "                    \n",
    "         \n",
    "        #####################\n",
    "        values_teen = interval_list_teen.values()\n",
    "        values_teen_vocabulary = interval_list_teen_vocabulary.values()\n",
    "        \n",
    "\n",
    "       \n",
    "        keys_teen= []\n",
    "        for i in interval_list_teen:\n",
    "            keys_teen.append(datetime.datetime.fromtimestamp(i))\n",
    "\n",
    "        values_array = np.array(list(values_teen))\n",
    "        values_array = values_array.reshape(-1,1)\n",
    "\n",
    "        values_array_vocabulary = np.array(list(values_teen_vocabulary))\n",
    "        values_array_vocabulary = values_array_vocabulary.reshape(-1,1)\n",
    "        values_array = values_array.reshape(-1,1)\n",
    "\n",
    "        print(values_array, values_array_vocabulary)\n",
    "        lin_regression = LinearRegression().fit(values_array, values_array_vocabulary)\n",
    "        model_line = lin_regression.predict(values_array)\n",
    "        \n",
    "        lower_vet = []\n",
    "        upper_vet = []\n",
    "        for m in model_line:\n",
    "            lower, prediction, upper =  self._get_prediction_interval(m,values_array_vocabulary , model_line)\n",
    "            lower_vet.append(lower)\n",
    "            upper_vet.append(upper)\n",
    "\n",
    "        print(values_teen)\n",
    "        print(values_teen_vocabulary)\n",
    "    \n",
    "\n",
    "        plt.figure\n",
    "        plt.plot(keys_teen,values_teen,color ='red')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "        plt.figure\n",
    "        plt.plot(keys_teen,values_teen_vocabulary,color ='blue')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "        print(values_array)\n",
    "        plt.figure\n",
    "        \n",
    "        plt.plot(values_array,values_array_vocabulary,color ='green')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "       \n",
    "  \n",
    "#ShowDialogAct().show_dialog_act()\n",
    "class ZipfTestingWithNPSChatCorpora:\n",
    "    ChatCorpora() #creates adult and teen dictionaries\n",
    "    \n",
    "    #1\n",
    "    '''\n",
    "    ChatCorpora()\n",
    "    teens = ZipfFrequenceData().get_frequence_data_words(corpus_teen[\"words\"])\n",
    "    zipf = ZipfFrequenceData().get_zipf_data(len(teens))\n",
    "    ShowData().show_data(teens, zipf)\n",
    "    \n",
    "    adults = ZipfFrequenceData().get_frequence_data_words(corpus_adult[\"words\"])\n",
    "    zipf = ZipfFrequenceData().get_zipf_data(len(adults))\n",
    "    ShowData().show_data(teens, zipf)\n",
    "    '''\n",
    "\n",
    "    #2\n",
    "    '''\n",
    "    teens_sentences = ZipfFrequenceData().get_frequence_data_sentence(corpus_teen[\"sentences\"])\n",
    "    zipf = ZipfFrequenceData().get_zipf_data(len(teens_sentences))\n",
    "    ShowData().show_data(teens_sentences, zipf)\n",
    "\n",
    "    adult_sentences = ZipfFrequenceData().get_frequence_data_sentence(corpus_adult[\"sentences\"])\n",
    "    zipf = ZipfFrequenceData().get_zipf_data(len(adult_sentences))\n",
    "    ShowData().show_data(adult_sentences, zipf)\n",
    "    '''\n",
    "\n",
    "    #3\n",
    "    #ShowWordCloud().show_word_cloud(corpus_adult[\"words\"])\n",
    "\n",
    "    #4\n",
    "    #EmpathClient().show_empath_tables()\n",
    "\n",
    "    #5\n",
    "    #LDAModel().print_keywords(10)\n",
    "    #print(corpus_teen[\"topics\"])\n",
    "\n",
    "    #6\n",
    "    #VaderScore().display_vader_score()\n",
    "    \n",
    "    #7\n",
    "    CoherenceOfCorpus().coherence()\n",
    "\n",
    "    #8\n",
    "    #ShowDialogAct().show_dialog_act()\n",
    "    \n",
    "\n",
    "    #TimelyDescretion().timely_descretion()\n",
    "    \n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ab8d1-fcfe-4540-a312-959c95fea22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac8c73-1360-4771-b88a-33addf19f058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
