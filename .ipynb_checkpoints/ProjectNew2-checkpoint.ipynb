{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca744ec-873a-4fdf-aff4-9c4ec232b50d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/r3pnjytj3056tzyf_rpysh7h0000gn/T/ipykernel_53907/1746343280.py:14: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el539076294539616967236570\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el539076294539616967236570_data = {\"mdsDat\": {\"x\": [-0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0], \"y\": [0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [84.999977061527, 1.6666692153858895, 1.6666692153858895, 1.6666692153858895, 1.6666692153858895, 1.6666692153858895, 1.6666692153858895, 1.6666692153858895, 1.6666692153858895, 1.6666692153858895]}, \"tinfo\": {\"Term\": [\"u\", \"lol\", \"hi\", \"ok\", \"lmao\", \"u\", \"lol\", \"hi\", \"ok\", \"lmao\", \"lmao\", \"ok\", \"hi\", \"u\", \"lol\", \"lmao\", \"hi\", \"lol\", \"ok\", \"u\", \"ok\", \"lmao\", \"hi\", \"lol\", \"u\", \"lol\", \"lmao\", \"hi\", \"u\", \"ok\", \"ok\", \"lmao\", \"u\", \"hi\", \"lol\", \"ok\", \"lmao\", \"lol\", \"u\", \"hi\", \"lmao\", \"ok\", \"hi\", \"lol\", \"u\", \"lmao\", \"ok\", \"lol\", \"u\", \"hi\", \"lmao\", \"ok\", \"hi\", \"lol\", \"u\"], \"Freq\": [1.0, 1.0, 1.0, 0.0, 0.0, 0.8500046707680964, 0.8500028342003141, 0.8500016309317671, 0.8499976411465848, 0.849992258103085, 0.01666693600554714, 0.016666786993694283, 0.01666672490542226, 0.01666654236590251, 0.016666484002926807, 0.016666871433744235, 0.016666876400805997, 0.016666662817150235, 0.01666649393705033, 0.016666554783556914, 0.01666704031384414, 0.01666667771833552, 0.016666605695939973, 0.0166665696847422, 0.01666657216827308, 0.01666696208262139, 0.016666795686052366, 0.016666716213064176, 0.016666588311223807, 0.0166664132222967, 0.016666948423201544, 0.01666680562017589, 0.016666768367212675, 0.016666497662346652, 0.016666452958790795, 0.016666876400805997, 0.016666774576039878, 0.016666642948903188, 0.016666608179470854, 0.016666560992384116, 0.016667075083276472, 0.01666661935535982, 0.016666662817150235, 0.016666640465372307, 0.01666647034350696, 0.016666801894879568, 0.016666823004892056, 0.01666665039949583, 0.01666664791596495, 0.016666549816495152, 0.016667113578005127, 0.016666688894224485, 0.016666657850088473, 0.016666539882371628, 0.01666646785997608], \"Total\": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0000038910631843, 1.0000024394426885, 1.000001483285464, 0.9999983316919543, 0.9999941096991412, 0.9999941096991412, 0.9999983316919543, 1.000001483285464, 1.0000038910631843, 1.0000024394426885, 0.9999941096991412, 1.000001483285464, 1.0000024394426885, 0.9999983316919543, 1.0000038910631843, 0.9999983316919543, 0.9999941096991412, 1.000001483285464, 1.0000024394426885, 1.0000038910631843, 1.0000024394426885, 0.9999941096991412, 1.000001483285464, 1.0000038910631843, 0.9999983316919543, 0.9999983316919543, 0.9999941096991412, 1.0000038910631843, 1.000001483285464, 1.0000024394426885, 0.9999983316919543, 0.9999941096991412, 1.0000024394426885, 1.0000038910631843, 1.000001483285464, 0.9999941096991412, 0.9999983316919543, 1.000001483285464, 1.0000024394426885, 1.0000038910631843, 0.9999941096991412, 0.9999983316919543, 1.0000024394426885, 1.0000038910631843, 1.000001483285464, 0.9999941096991412, 0.9999983316919543, 1.000001483285464, 1.0000024394426885, 1.0000038910631843], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [5.0, 4.0, 3.0, 2.0, 1.0, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6095, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6095, -1.6094, -1.6094, -1.6094, -1.6094, -1.6095, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6095, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6095], \"loglift\": [5.0, 4.0, 3.0, 2.0, 1.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0]}, \"token.table\": {\"Topic\": [1, 1, 1, 1, 1], \"Freq\": [0.9999985167167361, 1.0000058903355546, 0.9999975605632624, 1.000001668310829, 0.999996108951956], \"Term\": [\"hi\", \"lmao\", \"lol\", \"ok\", \"u\"]}, \"R\": 5, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 3, 4, 5, 6, 7, 8, 9, 10]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el539076294539616967236570\", ldavis_el539076294539616967236570_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"/files/d3.v5.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"/files/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el539076294539616967236570\", ldavis_el539076294539616967236570_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"/files/d3.v5.min.js\", function(){\n",
       "         LDAvis_load_lib(\"/files/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el539076294539616967236570\", ldavis_el539076294539616967236570_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=         x    y  topics  cluster       Freq\n",
       "topic                                      \n",
       "1     -0.0  0.0       1        1  84.999977\n",
       "0      0.0 -0.0       2        1   1.666669\n",
       "2     -0.0 -0.0       3        1   1.666669\n",
       "3      0.0  0.0       4        1   1.666669\n",
       "4     -0.0  0.0       5        1   1.666669\n",
       "5      0.0  0.0       6        1   1.666669\n",
       "6      0.0  0.0       7        1   1.666669\n",
       "7     -0.0 -0.0       8        1   1.666669\n",
       "8      0.0  0.0       9        1   1.666669\n",
       "9     -0.0 -0.0      10        1   1.666669, topic_info=   Term      Freq     Total Category  logprob  loglift\n",
       "4     u  1.000000  1.000000  Default   5.0000      5.0\n",
       "2   lol  1.000000  1.000000  Default   4.0000      4.0\n",
       "0    hi  1.000000  1.000000  Default   3.0000      3.0\n",
       "3    ok  0.000000  0.000000  Default   2.0000      2.0\n",
       "1  lmao  0.000000  0.000000  Default   1.0000      1.0\n",
       "4     u  0.850005  1.000004   Topic1  -1.6094      0.0\n",
       "2   lol  0.850003  1.000002   Topic1  -1.6094      0.0\n",
       "0    hi  0.850002  1.000001   Topic1  -1.6094      0.0\n",
       "3    ok  0.849998  0.999998   Topic1  -1.6094     -0.0\n",
       "1  lmao  0.849992  0.999994   Topic1  -1.6094     -0.0\n",
       "1  lmao  0.016667  0.999994   Topic2  -1.6094      0.0\n",
       "3    ok  0.016667  0.999998   Topic2  -1.6094      0.0\n",
       "0    hi  0.016667  1.000001   Topic2  -1.6094      0.0\n",
       "4     u  0.016667  1.000004   Topic2  -1.6094     -0.0\n",
       "2   lol  0.016666  1.000002   Topic2  -1.6095     -0.0\n",
       "1  lmao  0.016667  0.999994   Topic3  -1.6094      0.0\n",
       "0    hi  0.016667  1.000001   Topic3  -1.6094      0.0\n",
       "2   lol  0.016667  1.000002   Topic3  -1.6094     -0.0\n",
       "3    ok  0.016666  0.999998   Topic3  -1.6094     -0.0\n",
       "4     u  0.016667  1.000004   Topic3  -1.6094     -0.0\n",
       "3    ok  0.016667  0.999998   Topic4  -1.6094      0.0\n",
       "1  lmao  0.016667  0.999994   Topic4  -1.6094      0.0\n",
       "0    hi  0.016667  1.000001   Topic4  -1.6094     -0.0\n",
       "2   lol  0.016667  1.000002   Topic4  -1.6094     -0.0\n",
       "4     u  0.016667  1.000004   Topic4  -1.6094     -0.0\n",
       "2   lol  0.016667  1.000002   Topic5  -1.6094      0.0\n",
       "1  lmao  0.016667  0.999994   Topic5  -1.6094      0.0\n",
       "0    hi  0.016667  1.000001   Topic5  -1.6094     -0.0\n",
       "4     u  0.016667  1.000004   Topic5  -1.6094     -0.0\n",
       "3    ok  0.016666  0.999998   Topic5  -1.6095     -0.0\n",
       "3    ok  0.016667  0.999998   Topic6  -1.6094      0.0\n",
       "1  lmao  0.016667  0.999994   Topic6  -1.6094      0.0\n",
       "4     u  0.016667  1.000004   Topic6  -1.6094      0.0\n",
       "0    hi  0.016666  1.000001   Topic6  -1.6094     -0.0\n",
       "2   lol  0.016666  1.000002   Topic6  -1.6095     -0.0\n",
       "3    ok  0.016667  0.999998   Topic7  -1.6094      0.0\n",
       "1  lmao  0.016667  0.999994   Topic7  -1.6094      0.0\n",
       "2   lol  0.016667  1.000002   Topic7  -1.6094     -0.0\n",
       "4     u  0.016667  1.000004   Topic7  -1.6094     -0.0\n",
       "0    hi  0.016667  1.000001   Topic7  -1.6094     -0.0\n",
       "1  lmao  0.016667  0.999994   Topic8  -1.6094      0.0\n",
       "3    ok  0.016667  0.999998   Topic8  -1.6094     -0.0\n",
       "0    hi  0.016667  1.000001   Topic8  -1.6094     -0.0\n",
       "2   lol  0.016667  1.000002   Topic8  -1.6094     -0.0\n",
       "4     u  0.016666  1.000004   Topic8  -1.6095     -0.0\n",
       "1  lmao  0.016667  0.999994   Topic9  -1.6094      0.0\n",
       "3    ok  0.016667  0.999998   Topic9  -1.6094      0.0\n",
       "2   lol  0.016667  1.000002   Topic9  -1.6094     -0.0\n",
       "4     u  0.016667  1.000004   Topic9  -1.6094     -0.0\n",
       "0    hi  0.016667  1.000001   Topic9  -1.6094     -0.0\n",
       "1  lmao  0.016667  0.999994  Topic10  -1.6094      0.0\n",
       "3    ok  0.016667  0.999998  Topic10  -1.6094      0.0\n",
       "0    hi  0.016667  1.000001  Topic10  -1.6094     -0.0\n",
       "2   lol  0.016667  1.000002  Topic10  -1.6094     -0.0\n",
       "4     u  0.016666  1.000004  Topic10  -1.6095     -0.0, token_table=      Topic      Freq  Term\n",
       "term                       \n",
       "0         1  0.999999    hi\n",
       "1         1  1.000006  lmao\n",
       "2         1  0.999998   lol\n",
       "3         1  1.000002    ok\n",
       "4         1  0.999996     u, R=5, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 3, 4, 5, 6, 7, 8, 9, 10])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import nps_chat\n",
    "import re\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as pp\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import empath\n",
    "import sys\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "import datetime\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "corpus_teen = {\"words\":[], \"tagged_words\":[],\"tagged_sentences\":[], \"sentences\":[],\"topics\":[],\"dialogs\":[]}\n",
    "corpus_adult = {\"words\":[],\"tagged_words\":[],\"tagged_sentences\":[], \"sentences\":[],\"topics\":[], \"dialogs\":[]}\n",
    "\n",
    "class ChatCorpora:\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "    #personalpronoun, Proper noun, singular, Proper noun, plural,Possessive pronoun\n",
    "    pronouns = {\"NNP\", \"PRP\",\"NNP\", \"PRP$\"}\n",
    "    adjectives = {\"ADJ\"}\n",
    "\n",
    "    def __init__(self):\n",
    "       self._get_chat_data()\n",
    "            \n",
    "    \n",
    "    def _preprocess(self,list1):\n",
    "        \n",
    "        flat_list = []\n",
    "        if not isinstance(list1, list):\n",
    "            flat_list.append(list1)\n",
    "        else:\n",
    "            for sublist in list1:\n",
    "                if not isinstance(sublist, list):\n",
    "                    flat_list.append(sublist)\n",
    "                else:\n",
    "                    for item in sublist:\n",
    "                        flat_list.append(item)\n",
    "        \n",
    "        flat_list2 = [word for word in flat_list if re.match(r'[^\\W\\d]*$', word) and word[0] != 'JOIN' and word[0] != 'PART' and word[0] != 'ACTION' and word[0] not in stop_words ]\n",
    "        return flat_list2\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_chat_data(self):\n",
    "       \n",
    "        for i, p in enumerate(nps_chat.xml_posts()):\n",
    "           \n",
    "            if p.get(\"user\").find(\"teen\") != -1 :\n",
    "                \n",
    "                corpus_teen[\"tagged_sentences\"].append(list((t.get(\"word\"), t.get(\"pos\")) for t in p[0])) \n",
    "            else:             \n",
    "                corpus_adult[\"tagged_sentences\"].append(list((t.get(\"word\"), t.get(\"pos\")) for t in p[0]))  \n",
    "\n",
    "\n",
    "        \n",
    "        for sen in corpus_teen[\"tagged_sentences\"]:\n",
    "            for word in sen:\n",
    "                \n",
    "                if re.match(r'[^\\W\\d]*$', word[0]) and word[0] != 'JOIN' and word[0] != 'PART' and word[0] != 'ACTION' and word[0] not in stop_words :                \n",
    "                    corpus_teen[\"tagged_words\"].append(word)\n",
    "                    corpus_teen[\"words\"].append(word[0].lower())\n",
    "  \n",
    "        for sen in corpus_adult[\"tagged_sentences\"]:\n",
    "            for word in sen:\n",
    "                if re.match(r'[^\\W\\d]*$', word[0]) and word[0] != 'JOIN' and word[0] != 'PART' and word[0] != 'ACTION' and word[0] not in stop_words:\n",
    "                    corpus_adult[\"tagged_words\"].append(word)\n",
    "                    corpus_adult[\"words\"].append(word[0].lower())\n",
    "\n",
    "\n",
    "        \n",
    "        sentence_teen = {\"sentence\":\"\",\"pronouns\":0, \"adjectives\": 0}\n",
    "        \n",
    "        for sen in corpus_teen[\"tagged_sentences\"]:\n",
    "            if re.match(r'[^\\W\\d]*$', sen[0][0]) and sen[0][0] != 'JOIN' and sen[0][0] != 'PART' and sen[0][0] != 'ACTION'  :\n",
    "                sentence_teen[\"sentence\"] = sen\n",
    "                pro = 0\n",
    "                adj = 0\n",
    "                for word in sen:\n",
    "                    if word[1] in self.pronouns:\n",
    "                        pro +=1\n",
    "                    if word[1] in self.adjectives:\n",
    "                        adj +=1\n",
    "                corpus_teen[\"sentences\"].append({\"sentence\":sen,\"pronouns\":pro, \"adjectives\": adj})\n",
    "              \n",
    "        sentence_adult = {\"sentence\":\"\",\"pronouns\":0, \"adjectives\": 0}\n",
    "        \n",
    "        for sen in corpus_adult[\"tagged_sentences\"]:\n",
    "            if re.match(r'[^\\W\\d]*$', sen[0][0]) and sen[0][0] != 'JOIN' and sen[0][0] != 'PART' and sen[0][0] != 'ACTION'   :\n",
    "                sentence_adult[\"sentence\"] = sen\n",
    "                pro = 0\n",
    "                adj = 0\n",
    "                for word in sen:\n",
    "                    if word[1] in self.pronouns:\n",
    "                        pro +=1\n",
    "                    if word[1] in self.adjectives:\n",
    "                        adj +=1\n",
    "                corpus_adult[\"sentences\"].append({\"sentence\":sen,\"pronouns\":pro, \"adjectives\": adj})\n",
    "     \n",
    "               \n",
    "\n",
    "class ZipfFrequenceData:\n",
    "\n",
    "    \n",
    "    def get_frequence_data_words(self, corpus):\n",
    "        \n",
    "        distribution = defaultdict(int)\n",
    "        \n",
    "\n",
    "        for word in corpus:\n",
    "            distribution[word]+=1\n",
    "        \n",
    "        np.seterr(divide = 'ignore') \n",
    "        distribution1 = sorted(distribution.items(), key=lambda x:x[1], reverse = True)\n",
    "        max_val = distribution1[0][1]\n",
    "  \n",
    "        distribution = {}\n",
    "      \n",
    "        for val in distribution1:\n",
    "            distribution[val[0]] = val[1]/max_val     \n",
    "        return distribution\n",
    "\n",
    "    \n",
    "    def get_frequence_data_sentence(self,corpus_dict):\n",
    "        \n",
    "        distribution = defaultdict(int)\n",
    "\n",
    "        for sen in corpus_dict:\n",
    "            \n",
    "            distribution[str(sen[\"sentence\"])] = len(sen[\"sentence\"])\n",
    "            \n",
    "       \n",
    "        np.seterr(divide = 'ignore') \n",
    "        distribution1 = sorted(distribution.items(), key=lambda x:x[1], reverse = True)\n",
    "        max_val = distribution1[0][1]\n",
    "  \n",
    "        distribution = {}\n",
    "      \n",
    "        for val in distribution1:\n",
    "            distribution[val[0]] = val[1]/max_val     \n",
    "        return distribution\n",
    "\n",
    "\n",
    "    def get_frequence_data_pronouns(self,corpus_dict):\n",
    "        distribution = defaultdict(int)\n",
    "\n",
    "        for sen in corpus_dict:\n",
    "            distribution[str(sen[\"sentence\"])] = sen[\"pronouns\"]\n",
    "\n",
    "        \n",
    "            \n",
    "       \n",
    "        np.seterr(divide = 'ignore') \n",
    "        distribution1 = sorted(distribution.items(), key=lambda x:x[1], reverse = True)\n",
    "        max_val = distribution1[0][1]\n",
    "  \n",
    "        distribution = {}\n",
    "      \n",
    "        for val in distribution1:\n",
    "            distribution[val[0]] = val[1]/max_val     \n",
    "        return distribution\n",
    "\n",
    "    def get_frequence_data_adjectives(self,corpus_dict):\n",
    "        distribution = defaultdict(int)\n",
    "\n",
    "        for sen in corpus_dict:\n",
    "            distribution[str(sen[\"sentence\"])] = sen[\"adjectives\"]\n",
    "\n",
    "        \n",
    "            \n",
    "       \n",
    "        np.seterr(divide = 'ignore') \n",
    "        distribution1 = sorted(distribution.items(), key=lambda x:x[1], reverse = True)\n",
    "        max_val = distribution1[0][1]\n",
    "  \n",
    "        distribution = {}\n",
    "      \n",
    "        for val in distribution1:\n",
    "            distribution[val[0]] = val[1]/max_val     \n",
    "        return distribution\n",
    "        \n",
    "\n",
    "    def get_zipf_data(self,count):\n",
    "        result = []\n",
    "        for i in range(count):\n",
    "            result.append(1/(i +1))\n",
    "        return result\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "class ShowData:\n",
    "    \n",
    "    def _get_prediction_interval(self, model_line_point, y_data, model_line, pi=.90):\n",
    "       \n",
    "        \n",
    "        sum_errs = np.sum((y_data - model_line)**2)\n",
    "        stdev = np.sqrt(1 / (len(y_data) - 2) * sum_errs)\n",
    "        one_minus_pi = 1 - pi\n",
    "        ppf_lookup = 1 - (one_minus_pi / 2)\n",
    "        z_score = stats.norm.ppf(ppf_lookup)\n",
    "        interval = z_score * stdev\n",
    "\n",
    "        lower, upper = model_line_point - interval, model_line_point + interval\n",
    "        return lower, model_line_point, upper\n",
    " \n",
    "\n",
    "   \n",
    "\n",
    "    def _tokens_outside_bounds(self, data, upper_interval, lower_interval):\n",
    "\n",
    "        data2 = []\n",
    "        sum2 = 0\n",
    "        sum_all = 0\n",
    "        \n",
    "        for i, d in enumerate(data):\n",
    "            sum_all += data[i]\n",
    "            #print(\"DATA_____: \", d, upper_interval[i], lower_interval[i])\n",
    "            if (data[i] > upper_interval[i])| (data[i] < lower_interval[i]):\n",
    "                data2.append(data[i])\n",
    "                sum2 += data[i]\n",
    "\n",
    "        outside_values = sum2\n",
    "\n",
    "        \n",
    "        result = outside_values/sum_all * 100\n",
    "        print(\"Result: \", result)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    \n",
    "    \n",
    "    def show_data(self, distribution, zipf, name):\n",
    "  \n",
    "        \n",
    "        data_y = list(distribution.values())\n",
    "        data_x = np.arange(1,len(data_y) + 1,1)\n",
    "      \n",
    "\n",
    "      \n",
    "        lower_interval = []\n",
    "        upper_interval = []\n",
    "        \n",
    "       \n",
    "        log_x = np.log10(data_x)\n",
    "        log_y = np.log10(data_y)\n",
    "        \n",
    "        \n",
    "        log_x_composed = log_x.reshape(-1,1)\n",
    "       \n",
    "        lin_regression = LinearRegression().fit(log_x_composed, log_y)\n",
    "        model_line = lin_regression.predict(log_x_composed)\n",
    "        \n",
    "        \n",
    "        for m in model_line:\n",
    "            lower, prediction, upper =  self._get_prediction_interval(m,log_y, model_line)\n",
    "            lower_interval.append(lower)\n",
    "            upper_interval.append(upper)\n",
    "    \n",
    "        \n",
    "        plt.figure\n",
    "        #plt.axes().set_aspect('equal')\n",
    "        plt.xlabel('Rangking')\n",
    "        plt.ylabel('Frequencies')\n",
    "        #plt.plot(data_x,data_y, color='green', label='Zipf loglog distribution')\n",
    "        \n",
    "        plt.title(f'Loglog distribution {name}' )\n",
    "        plt.plot(log_x,log_y, color='green', label='Zipf loglog distribution')\n",
    "        plt.plot(log_x, model_line, color='orange', label='Regression' )\n",
    "        print(len(log_x), len(upper_interval), len(lower_interval))\n",
    "        plt.fill_between(log_x,upper_interval,lower_interval , color='b',label='Confidence Interval', alpha=.2)\n",
    "        #plt.plot(np.arange(0,len(y_data),1),y_data,color='orange',label='Real data')\n",
    "        \n",
    "        print(self._tokens_outside_bounds( log_y, upper_interval, lower_interval))\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data = ZipfFrequenceData().get_frequence_data_words(corpus_adult[\"words\"])\n",
    "#data = ZipfFrequenceData().get_frequence_data_sentence(corpus_adult[\"sentences\"])\n",
    "#zipf = ZipfFrequenceData().get_zipf_data(len(data))\n",
    "#ShowData().show_data(data,zipf)\n",
    "\n",
    "\n",
    "\n",
    "#3. Wordcloud\n",
    "\n",
    "\n",
    " \n",
    "# Reads 'Youtube04-Eminem.csv' file \n",
    "\n",
    "\n",
    "class ShowWordCloud:\n",
    "    def show_word_cloud(self, corpus):\n",
    "        stopwords = set(STOPWORDS)\n",
    "\n",
    "     \n",
    "        comment_words = \" \".join(corpus)+\" \"\n",
    "         \n",
    "        wordcloud = WordCloud(width = 800, height = 800,\n",
    "                        background_color ='white',\n",
    "                        stopwords = stopwords,\n",
    "                        min_font_size = 10).generate(comment_words)\n",
    "         \n",
    "        # plot the WordCloud image                       \n",
    "        plt.figure(figsize = (8, 8), facecolor = None)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad = 0)\n",
    "         \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#ShowWordCloud().show_word_cloud(corpus_adult[\"words\"])\n",
    "\n",
    "#4\n",
    "class EmpathClient:\n",
    "\n",
    "    def show_empath_tables(self):\n",
    "        lexicon = empath.Empath()\n",
    "    \n",
    "        adult_chat = corpus_adult[\"words\"]\n",
    "        teen_chat = corpus_teen[\"words\"]\n",
    "        \n",
    "        adult_categories = lexicon.analyze(adult_chat, normalize=True)\n",
    "        teen_categories = lexicon.analyze(teen_chat, normalize=True)\n",
    "        \n",
    "        def top_categories(categories, n=10):\n",
    "          return sorted(categories.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "        \n",
    "        #print(top_categories(adult_categories, 30))\n",
    "        #print(top_categories(teen_categories, 30))\n",
    "        adult_data = (top_categories(adult_categories, 30))\n",
    "        teen_data = (top_categories(teen_categories, 30))\n",
    "       \n",
    "        df_teen = pd.DataFrame(teen_data, columns = [\"categories\", \"freuency\"])\n",
    "        df_adult = pd.DataFrame(adult_data, columns = [\"categories\", \"freuency\"])\n",
    "\n",
    "       \n",
    "     \n",
    "        #print(df_teen)\n",
    "        #print(df_adult)\n",
    "\n",
    "        print(df_teen.compare(df_adult))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#EmpathClient().show_empath_tables()\n",
    "\n",
    "#5\n",
    "\n",
    "class LDAModel:\n",
    "   \n",
    "    def print_keywords(self,num_of_topics, dataset):\n",
    "        \n",
    "        #print(dataset)\n",
    "        data_words = [dataset]#[d.split() for d in dataset]\n",
    "   \n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "        # Create Corpus\n",
    "        texts = data_words\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "        # View\n",
    "        #print(corpus[:1][0][:30])\n",
    "        \n",
    "        \n",
    "        # number of topics\n",
    "        num_topics = num_of_topics\n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics)\n",
    "        topic_list = []\n",
    "        # Print the Keyword in the 10 topics\n",
    "        for topic in lda_model.print_topics():\n",
    "            list1 = []\n",
    "            topics0 = topic[1].split('+')\n",
    "            for topic0 in topics0:\n",
    "                topic1 = topic0.split('*')\n",
    "                #print(topic1[1])\n",
    "                topic1[1] = topic1[1].replace('\"','')\n",
    "                list1.append(topic1[1].strip())\n",
    "            \n",
    "            \n",
    "            topic_list.append(list1)\n",
    "        t_dict = {}\n",
    "       \n",
    "          \n",
    "        corpus_teen[\"topics\"] = topic_list\n",
    "        doc_lda = lda_model[corpus]\n",
    "        self.show_LDA_data( lda_model, corpus, id2word, num_topics)\n",
    "        #print(corpus_teen[\"topics\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def show_LDA_data(self, lda_model, corpus, id2word, num_topics):\n",
    "        \n",
    "           \n",
    "        # Visualize the topics\n",
    "        pyLDAvis.enable_notebook(local=True)\n",
    "        LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "        # # this is a bit time consuming - make the if statement True\n",
    "        # # if you want to execute visualization prep yourself\n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "        '''\n",
    "        if 1 == 1:\n",
    "            LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "            with open(LDAvis_data_filepath, 'wb') as f:\n",
    "                pickle.dump(LDAvis_prepared, f)\n",
    "        # load the pre-prepared pyLDAvis data from disk\n",
    "        with open(LDAvis_data_filepath, 'rb') as f:\n",
    "            LDAvis_prepared = pickle.load(f)\n",
    "        pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "        '''\n",
    "        display(LDAvis_prepared)\n",
    "\n",
    "\n",
    "#6\n",
    "class VaderScore:\n",
    "    \n",
    "    def display_vader_score(self):\n",
    "        LDAModel().print_keywords(10)\n",
    "        \n",
    "        text = corpus_teen[\"topics\"]\n",
    "        if text == []:\n",
    "            text = LDAModel().print_keywords(10)\n",
    "        \n",
    "        df = text\n",
    "        text2 = []\n",
    "        for t in text:\n",
    "            t = ' '.join(t)\n",
    "            text2.append(t)\n",
    "            #print(t)\n",
    "        #print(text2)\n",
    "        df = text2\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        vader_keywords = []\n",
    "        vader_score = []\n",
    "        \n",
    "        for t in text2:\n",
    "            #list2.append((t,sid.polarity_scores(t)))\n",
    "            #vader.append([t, sid.polarity_scores(t)])\n",
    "            vader_keywords.append(t)\n",
    "            vader_score.append(sid.polarity_scores(t))\n",
    "            #vader.append(vader)\n",
    "        #print(vader)\n",
    "        df_vader= pd.DataFrame( {\"keywords\": vader_keywords,\"score\": vader_score})\n",
    "        #print(df_vader)\n",
    "        display(df_vader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#7\n",
    "class CoherenceOfCorpus:\n",
    "\n",
    "    \n",
    "    def coherence(self):\n",
    "        ChatCorpora()\n",
    "        corpus = corpus_teen[\"words\"]\n",
    "        LDAModel().print_keywords(10)\n",
    "        topics_list = corpus_teen[\"topics\"]\n",
    "        \n",
    "        for topic in topics_list:\n",
    "            print(topic)\n",
    "        \n",
    "        word_coherence = {}\n",
    "        for word in corpus:\n",
    "            word_coherence[word] = 0\n",
    "\n",
    "        corpus1 = list(set(corpus))\n",
    "        for word in corpus1:\n",
    "            \n",
    "            for i, topics in enumerate(topics_list):\n",
    "                  \n",
    "                if word.strip() in list(set(topics_list[i])):\n",
    "                    word_coherence[word]+=1\n",
    "        #print(word_coherence.keys())\n",
    "        values = list(word_coherence.values())\n",
    "        values2 = []\n",
    "        for v in values:\n",
    "            v = (v-1)/10\n",
    "            values2.append(v)\n",
    "        #print(values2)\n",
    "        max1 = max(values2)\n",
    "        print(max1)\n",
    "        #print(word_coherence)\n",
    "\n",
    "        coherence = 1-max1\n",
    "        return coherence\n",
    "        \n",
    "                    \n",
    "                                    \n",
    "                        \n",
    "              \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#8\n",
    "class ShowDialogAct:\n",
    "\n",
    "    def __init__(self):\n",
    "       \n",
    "        \n",
    "        self.dialogs_teen ={\"Emotion\":0, \"ynAnswer\":0, \"Continuer\":0, \"WhQuestion\":0, \"System\":0, \"Accept\":0, \"Clarity\":0\n",
    "                     , \"Emphasis\":0, \"nAnswer\":0, \"Great\":0, \"Statement\":0, \"Reject\":0, \"Bye\":0, \"Others\":0\n",
    "            }\n",
    "        self.dialogs_adult ={\"Emotion\":0, \"ynAnswer\":0, \"Continuer\":0, \"WhQuestion\":0, \"System\":0, \"Accept\":0, \"Clarity\":0\n",
    "                     , \"Emphasis\":0, \"nAnswer\":0, \"Great\":0, \"Statement\":0, \"Reject\":0, \"Bye\":0, \"Others\":0\n",
    "            }\n",
    "    \n",
    "    \n",
    "    def show_dialog_act(self):\n",
    "        #print(corpus_teen[\"sentences\"])\n",
    "        for i, p in enumerate(nps_chat.xml_posts()):\n",
    "               \n",
    "            if p.get(\"user\").find(\"teen\") != -1 :\n",
    "                da = p.get(\"class\")\n",
    "                test = da in self.dialogs_teen.keys()\n",
    "                #print(dialogs_teen.keys(), da, test )\n",
    "                if da in self.dialogs_teen.keys():\n",
    "                    self.dialogs_teen[da] +=1\n",
    "                    #if da == \"Bye\":\n",
    "                        #print(da, dialogs_teen[da])\n",
    "            else:\n",
    "                da = p.get(\"class\")\n",
    "                test = da in self.dialogs_adult.keys()\n",
    "                    #print(dialogs_teen.keys(), da, test )\n",
    "                if da in self.dialogs_adult.keys():\n",
    "                    self.dialogs_adult[da] +=1\n",
    "                    #if da == \"Bye\":\n",
    "                            #print(da, dialogs_adult[da])\n",
    "\n",
    "        \n",
    "        print(self.dialogs_teen.values())\n",
    "        plt.figure\n",
    "        plt.bar(self.dialogs_teen.keys(),self.dialogs_teen.values(),color ='red',\n",
    "        width = 0.4)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "        print(self.dialogs_adult.values())\n",
    "        plt.figure\n",
    "        plt.bar(self.dialogs_adult.keys(),self.dialogs_adult.values(),color ='blue',\n",
    "        width = 0.4)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "        corpus_teen[\"dialogs\"] = self.dialogs_teen\n",
    "        corpus_adult[\"dialogs\"] = self.dialogs_adult\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "teen_dates = {}\n",
    "adult_dates = {}\n",
    "teen_dates_vocabulary = {}\n",
    "teen_vocabulary_set = set()\n",
    "\n",
    "#9 & 10\n",
    "class TimelyDescretion:\n",
    "\n",
    "    def _get_prediction_interval(self, model_line_point, y_data, model_line, pi=.90):\n",
    "       \n",
    "        \n",
    "        sum_errs = np.sum((y_data - model_line)**2)\n",
    "        stdev = np.sqrt(1 / (len(y_data) - 2) * sum_errs)\n",
    "        one_minus_pi = 1 - pi\n",
    "        ppf_lookup = 1 - (one_minus_pi / 2)\n",
    "        z_score = stats.norm.ppf(ppf_lookup)\n",
    "        interval = z_score * stdev\n",
    "\n",
    "        lower, upper = model_line_point - interval, model_line_point + interval\n",
    "        return lower, model_line_point, upper\n",
    " \n",
    "\n",
    "\n",
    "    def _preprocess(self, list1):\n",
    "        \n",
    "        flat_list = []\n",
    "        if not isinstance(list1, list):\n",
    "            flat_list.append(list1)\n",
    "        else:\n",
    "            for sublist in list1:\n",
    "                if not isinstance(sublist, list):\n",
    "                    flat_list.append(sublist)\n",
    "                else:\n",
    "                    for item in sublist:\n",
    "                        flat_list.append(item)\n",
    "            \n",
    "        flat_list2 = [word for word in flat_list if re.match(r'[^\\W\\d]*$', word) and word != 'JOIN' and word != 'PART' and word not in \n",
    "                      stop_words]\n",
    "        return flat_list2\n",
    "\n",
    "   \n",
    "    \n",
    "    def get_interval(self,list1, b, c):\n",
    "        list1 = sorted(list1)\n",
    "        d1 = datetime.datetime(2006, 1, 1).timestamp()\n",
    "        d2 = datetime.datetime(2006, 1, 2) .timestamp()\n",
    "        i0 = (d2-d1)\n",
    "\n",
    "        \n",
    "        t_min = list1[0] -b*i0\n",
    "        t_max = list1[-1] -b*i0\n",
    "       \n",
    "        interval =  c*i0\n",
    "        return t_min, t_max,interval\n",
    "\n",
    "    \n",
    "    def timely_descretion(self):\n",
    "        for p in nps_chat.xml_posts():\n",
    "            \n",
    "            datetime1 = datetime.datetime(2006, int(p.get(\"user\")[0:2]), int(p.get(\"user\")[3:5])).timestamp() \n",
    "            if p.get(\"user\").find(\"teen\") != -1 :\n",
    "            \n",
    "                teen_dates[datetime1] = 0\n",
    "                teen_dates_vocabulary[datetime1] = 0\n",
    "            else:\n",
    "                adult_dates[datetime1] = 0\n",
    "            \n",
    "\n",
    "   \n",
    "        for p in nps_chat.xml_posts():\n",
    "            \n",
    "            datetime1 = datetime.datetime(2006, int(p.get(\"user\")[0:2]), int(p.get(\"user\")[3:5])).timestamp()\n",
    "            if p.get(\"user\").find(\"teen\") != -1 :\n",
    "            \n",
    "                list1 = list(t.get(\"word\") for t in p[0]  if re.match(r'[^\\W\\d]*$', t.get(\"word\") ) \n",
    "                             and t.get(\"word\") != 'JOIN' and t.get(\"word\")  != 'PART' and t.get(\"word\")  != 'ACTION' and word not in stop_words)\n",
    "            \n",
    "                if list1 != []:\n",
    "                    list2 = self._preprocess(list1)\n",
    "                    \n",
    "                    teen_dates[datetime1] += len(list2)\n",
    "                    for t in list2:\n",
    "                        if t not in teen_vocabulary_set:\n",
    "                        \n",
    "                            teen_dates_vocabulary[datetime1] += 1\n",
    "                            teen_vocabulary_set.add(t) \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    \n",
    "               \n",
    "            else:\n",
    "               pass\n",
    "                \n",
    "            \n",
    "        startdate = 0\n",
    "        dayinterval = 7\n",
    "        #sorted_teen_dates = sorted(list(teen_dates_vocabulary.keys()))\n",
    "        min0,max0, interval = self.get_interval(teen_dates,startdate,dayinterval)\n",
    "       \n",
    "        \n",
    "        interval_list_teen = {}\n",
    "        interval_list_teen_vocabulary = {}\n",
    "        interval_values = 0\n",
    "        key0 = min0\n",
    "        while key0 < max0 :\n",
    "            interval_list_teen[key0] = 0\n",
    "            interval_list_teen_vocabulary[key0] = 0\n",
    "            key0 += interval\n",
    "\n",
    "        interval_keys = list(interval_list_teen.keys())\n",
    "        print(interval_keys)\n",
    "        \n",
    "        for d in teen_dates.keys():\n",
    "            for i, key  in enumerate(interval_keys):\n",
    "                \n",
    "                if i < len(interval_keys) -1:\n",
    "                    if d >= interval_keys[i] and d < interval_keys[i+1]:\n",
    "                    \n",
    "                        interval_list_teen[interval_keys[i]] += teen_dates[d]\n",
    "\n",
    "        for d in teen_dates_vocabulary.keys():\n",
    "            for i, key  in enumerate(interval_keys):\n",
    "                if i < len(interval_keys) -1:\n",
    "                    if d >= interval_keys[i] and d < interval_keys[i+1]:\n",
    "                        interval_list_teen_vocabulary[interval_keys[i]] += teen_dates_vocabulary[d]\n",
    "                    \n",
    "         \n",
    "        #####################\n",
    "        values_teen = interval_list_teen.values()\n",
    "        values_teen_vocabulary = interval_list_teen_vocabulary.values()\n",
    "        \n",
    "\n",
    "       \n",
    "        keys_teen= []\n",
    "        for i in interval_list_teen:\n",
    "            keys_teen.append(datetime.datetime.fromtimestamp(i))\n",
    "\n",
    "        values_array = np.array(list(values_teen))\n",
    "        values_array = values_array.reshape(-1,1)\n",
    "\n",
    "        values_array_vocabulary = np.array(list(values_teen_vocabulary))\n",
    "        values_array_vocabulary = values_array_vocabulary.reshape(-1,1)\n",
    "        values_array = values_array.reshape(-1,1)\n",
    "\n",
    "        print(values_array, values_array_vocabulary)\n",
    "        lin_regression = LinearRegression().fit(values_array, values_array_vocabulary)\n",
    "        model_line = lin_regression.predict(values_array)\n",
    "        \n",
    "        lower_interval = []\n",
    "        upper_interval = []\n",
    "        for m in model_line:\n",
    "            lower, prediction, upper =  self._get_prediction_interval(m,values_array_vocabulary , model_line)\n",
    "            lower_interval.append(lower)\n",
    "            upper_interval.append(upper)\n",
    "\n",
    "        print(values_teen)\n",
    "        print(values_teen_vocabulary)\n",
    "    \n",
    "\n",
    "        plt.figure\n",
    "        plt.plot(keys_teen,values_teen,color ='red')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "        plt.figure\n",
    "        plt.plot(keys_teen,values_teen_vocabulary,color ='blue')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "        print(values_array)\n",
    "        plt.figure\n",
    "        \n",
    "        plt.plot(values_array,values_array_vocabulary,color ='green')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "       \n",
    "  \n",
    "#ShowDialogAct().show_dialog_act()\n",
    "class ZipfTestingWithNPSChatCorpora:\n",
    "    ChatCorpora() #creates adult and teen dictionaries\n",
    "    \n",
    "    #1\n",
    "    '''\n",
    "    ChatCorpora()\n",
    "    teens = ZipfFrequenceData().get_frequence_data_words(corpus_teen[\"words\"])\n",
    "    zipf = ZipfFrequenceData().get_zipf_data(len(teens))\n",
    "    ShowData().show_data(teens, zipf,\"teen\")\n",
    "    \n",
    "    adults = ZipfFrequenceData().get_frequence_data_words(corpus_adult[\"words\"])\n",
    "    zipf = ZipfFrequenceData().get_zipf_data(len(adults))\n",
    "    ShowData().show_data(adults, zipf, \"adult\")\n",
    "    '''\n",
    "\n",
    "    #2\n",
    "    '''\n",
    "    teens_sentences = ZipfFrequenceData().get_frequence_data_sentence(corpus_teen[\"sentences\"])\n",
    "    zipf = ZipfFrequenceData().get_zipf_data(len(teens_sentences))\n",
    "    ShowData().show_data(teens_sentences, zipf, \"teen\")\n",
    "\n",
    "    \n",
    "    adult_sentences = ZipfFrequenceData().get_frequence_data_sentence(corpus_adult[\"sentences\"])\n",
    "   \n",
    "    zipf = ZipfFrequenceData().get_zipf_data(len(adult_sentences))\n",
    "    ShowData().show_data(adult_sentences, zipf,\"adult\")\n",
    "        \n",
    "    '''\n",
    "    #3\n",
    "    #ShowWordCloud().show_word_cloud(corpus_adult[\"words\"])\n",
    "\n",
    "    #4\n",
    "    #EmpathClient().show_empath_tables()\n",
    "\n",
    "    #5\n",
    "    #teen_data = [\"hey\",\"lol\",\"girl\",\"pm\",\"chat\"]\n",
    "    #adult_data = [\"lol\",\"hi\",\"ok\",\"lmao\",\"u\"]\n",
    "    #LDAModel().print_keywords(10,adult_data)\n",
    "    #print(corpus_teen[\"topics\"])\n",
    "\n",
    "    #6\n",
    "    VaderScore().display_vader_score()\n",
    "    \n",
    "    #7\n",
    "    #CoherenceOfCorpus().coherence()\n",
    "\n",
    "    #8\n",
    "    #ShowDialogAct().show_dialog_act()\n",
    "    \n",
    "\n",
    "    #TimelyDescretion().timely_descretion()\n",
    "    \n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf46888-010f-4203-9b70-4f21d30bdc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
